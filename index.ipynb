{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This lesson summarizes the topics we'll be covering in section 45 and why they'll be important to you as a data scientist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "You will be able to:\n",
    "* Understand and explain what is covered in this section\n",
    "* Understand and explain why the section will help you to become a data scientist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep NLP - Word Embeddings\n",
    "\n",
    "In this section, we'll dive deeper into the concept of natural language processing!\n",
    "\n",
    "### Word Embeddings\n",
    "\n",
    "In this section, you'll learn about the concept of Word Embeddings, and how you can use them to model the semantic meanings of words in a high-dimensional embedding space! Word embeddings use similarity metrics to represent how two words relate to each other. This way, we can understand the words in our corpus to a bigger extent. A typical example is the example of \"Man\" vs \"woman\" and \"king\" vs \"queen\": word embeddings can capture that the word \"man\" relates to the word \"woman\" the same way the word \"king\" reates to \"queen\"!\n",
    "\n",
    "\n",
    "### Using Word2Vec\n",
    "\n",
    "Creating word embeddings is not an easy task. Word Embeddings can be created using so-called \"Word2Vec\" models that are  given enough training data. At its core, Word2Vec is just another Deep Neural Network, that looks at sequences of words and words that are often used in similar contexts (or *close* to each other in sentences). In this section you'll learn how to train a Word2Vec model, and you'll explore the Embedding Space.\n",
    "\n",
    "\n",
    "### Classification with Word Embeddings\n",
    "\n",
    "To wrap up this section, we'll focus on the practical aspects of how Word2Vec and Word Embeddings can be used to improve our Text Classification models. We'll start by learning how Transfer Learning can be used by loading pre-trained word vectors into our Word2Vec model. Then, we'll learn about how we can get the word vectors we need and combine them into Mean Word Vectors, and how we can streamline this process by writing our own vectorizer class that is compatible with scikit-learn pipelines. Nexy, we'll see how Deep Neural Networks with their own Embedding Laye3rs can be trained, and how Keras preprocesses the text data to make everything run smoothly!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section, you'll dive deeper into NLP and get better classification results using Word Embeddings!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
